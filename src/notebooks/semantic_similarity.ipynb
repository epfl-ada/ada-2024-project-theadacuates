{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import statistics\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "\n",
    "os.chdir(\"/Users/rapha/EPFL/ADA/ada-2024-project-theadacuates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = pd.read_csv(\"data/output/base_data/all_articles_processed.csv\")\n",
    "\n",
    "df_paths['path'] = df_paths['path'].str.split(';')\n",
    "\n",
    "df_pf = df_paths.loc[df_paths[\"finished\"] == True]\n",
    "df_uf = df_paths.loc[df_paths[\"finished\"] == False]\n",
    "\n",
    "cols_to_convert = [\"path_list\", \"path_list_id\", \"resolved_path_list_id\", \"resolved_path_list_name\"]\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    df_paths[col] = df_paths[col].apply(ast.literal_eval)\n",
    "\n",
    "df_names = pd.read_csv(\"data/output/base_data/articles_processed.csv\")\n",
    "\n",
    "df_paths.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic similarity with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyse the paths using semantic similary with SentenceTransmormer, which allows us to convert text to multidimensional vectors that can then be compared by taking the angle between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model.similarity_fn_name = \"cosine\" # valid options are “cosine”, “dot”, “euclidean”, and \"manhattan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples1 = [\"William Shakespeare\", \"playwright\", \"Romeo and Juliet\", \"17th century\", \"love\", \"Data analysis\"]\n",
    "embeddings1 = model.encode(examples1)\n",
    "\n",
    "similarities = model.similarity(embeddings1, embeddings1)\n",
    "similarities = pd.DataFrame(similarities)\n",
    "similarities.columns = examples1\n",
    "similarities.index = examples1\n",
    "\n",
    "sns.heatmap(similarities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example to show the semantic similarity at work. The results obtained are about what we would expect, William Shakespeare being semantically close to playwright, or Romeo and Juliet, but having nothing to do with Data analysis. We can start to see how this could be used to analyse similarity between articles. Obviously this isn't perfect as it doesn't work for more complex relationships such as between \"Romeo and Juliet\" and \"love\" which one could expect to be closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix_path = \"data/output/semantic_similarity_data/similarity_matrix.csv\"\n",
    "\n",
    "def clamp(n, min, max): \n",
    "    if n < min: \n",
    "        return min\n",
    "    elif n > max: \n",
    "        return max\n",
    "    else: \n",
    "        return n \n",
    "\n",
    "if not os.path.exists(similarity_matrix_path):\n",
    "    embeddings = model.encode(df_names[\"article_name\"])\n",
    "    num_articles = len(embeddings)\n",
    "    matrix = np.zeros(shape=(num_articles, num_articles))\n",
    "\n",
    "    for i in range (0, num_articles):\n",
    "        similarities = model.similarity([embeddings[i]], embeddings[0:(i+1)])[0]\n",
    "        for j in range(0, i+1):\n",
    "            similarity = clamp(similarities[j], 0, 1)\n",
    "            matrix[i][j] = similarity\n",
    "            if i != j:\n",
    "                matrix[j][i] = similarity\n",
    "\n",
    "    matrix = pd.DataFrame(matrix, index = df_names[\"article_name\"], columns=df_names[\"article_name\"])\n",
    "    matrix.to_csv(similarity_matrix_path)\n",
    "\n",
    "similarity_matrix = pd.read_csv(similarity_matrix_path, sep=',', header=0, comment='#', index_col=\"article_name\")\n",
    "display(similarity_matrix)\n",
    "\n",
    "def SemanticSimilarity(article_from, article_to):\n",
    "    try:\n",
    "        return similarity_matrix[article_from][article_to]\n",
    "    except:\n",
    "        print(\"Cannot find simlarity between\", article_from, \"and\", article_to)\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic shift\n",
    "\n",
    "Semantic shift denotes the semantic distance between successive articles. Large semantic shifts between articles can represent a higher cognitive load for the player (source?). We will try to analyse this semantic shift along paths to see if this can have an impact on whether the target is reached or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSemanticShiftList(row):\n",
    "    path_list = row.resolved_path_list_name\n",
    "    semantic_shift_list = []\n",
    "\n",
    "    for i in range(len(path_list) - 1):\n",
    "        semantic_similarity = SemanticSimilarity(path_list[i], path_list[i+1])\n",
    "        # we want the shift, how different the two articles so take 1 - similarity\n",
    "        semantic_shift_list.append(1 - semantic_similarity)\n",
    "    \n",
    "    return semantic_shift_list\n",
    "\n",
    "df_paths[\"path_semantic_shift\"] = df_paths.apply(lambda row: CreateSemanticShiftList(row), axis = 1)\n",
    "df_paths[\"average_semantic_shift\"] = df_paths[\"path_semantic_shift\"].apply(lambda path: statistics.mean(path) if len(path) > 1 else np.nan)\n",
    "\n",
    "average_semantic_shift_pf = df_paths[df_paths[\"finished\"] == True][\"average_semantic_shift\"]\n",
    "average_semantic_shift_pf = [x for x in average_semantic_shift_pf if ~np.isnan(x)]\n",
    "\n",
    "average_semantic_shift_uf = df_paths[df_paths[\"finished\"] == False][\"average_semantic_shift\"]\n",
    "average_semantic_shift_uf = [x for x in average_semantic_shift_uf if ~np.isnan(x)]\n",
    "\n",
    "print(f\"Semantic shift (finished paths): mean = {statistics.mean(average_semantic_shift_pf):.4f} median = {statistics.median(average_semantic_shift_pf):.4f}\")\n",
    "print(f\"Semantic shift (unfinished paths): mean = {statistics.mean(average_semantic_shift_uf):.4f} median = {statistics.median(average_semantic_shift_uf):.4f}\")\n",
    "pval = ttest_ind(average_semantic_shift_pf, average_semantic_shift_uf).pvalue\n",
    "print(\"P-value = {} so the semantic shift between finished and unfinished paths is significantly different???????\".format(pval))\n",
    "\n",
    "sns.histplot(data=df_paths, x=\"average_semantic_shift\", hue=\"finished\", multiple=\"layer\", binwidth=0.02, binrange=[0,1], stat=\"proportion\", common_norm = False).set(title = \"Semantic shift of a path for finished vs unfinished paths\", xlabel = \"Average semantic shift of path\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that semantic shift along finished paths is smaller than for unfinished paths, which means finished paths tend to follow more closely related articles than unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSemanticSimilarityList(row):\n",
    "    path_list = row.resolved_path_list_name\n",
    "    similarity_list = []\n",
    "\n",
    "    for article in path_list:\n",
    "        similarity = SemanticSimilarity(article, row.target_link)\n",
    "    \n",
    "        similarity_list.append(similarity)\n",
    "    \n",
    "    return similarity_list\n",
    "\n",
    "df_paths[\"path_similarity\"] = df_paths.apply(lambda row: CreateSemanticSimilarityList(row), axis = 1)\n",
    "df_paths[\"average_similarity\"] = df_paths[\"path_similarity\"].apply(lambda path: statistics.mean(path) if len(path) > 0 else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 5\n",
    "\n",
    "df_uf_similarity = pd.DataFrame(index = [x for x in range (2, max_length + 1)], columns = [x for x in range (0, max_length)])\n",
    "df_pf_similarity = pd.DataFrame(index = [x for x in range (2, max_length + 1)], columns = [x for x in range (0, max_length)])\n",
    "\n",
    "for length in range (2, max_length + 1):\n",
    "    similarities = df_paths.loc[(df_paths[\"finished\"] == False) & (df_paths[\"n_click\"] == length)][\"path_similarity\"].tolist()\n",
    "    similarities = np.nanmean(np.array(similarities), axis=0).tolist()\n",
    "    similarities += [np.nan] * (max_length - length)\n",
    "    df_uf_similarity.iloc[length - 2] = similarities\n",
    "\n",
    "    similarities = df_paths.loc[(df_paths[\"finished\"] == True) & (df_paths[\"n_click\"] == length + 1)][\"path_similarity\"].tolist()\n",
    "    # remove last node, always max semantic similarity because reached target\n",
    "    similarities = [x[:-1] for x in similarities]\n",
    "    similarities = np.nanmean(np.array(similarities), axis=0).tolist()\n",
    "    similarities += [np.nan] * (max_length - length)\n",
    "    df_pf_similarity.iloc[length - 2] = similarities\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), sharey=True)\n",
    "fig.supxlabel(\"article number\")\n",
    "fig.supylabel(\"similarity to target article\")\n",
    "\n",
    "y = [x for x in range (max_length)]\n",
    "len_path = 2\n",
    "for it, row in df_uf_similarity.iterrows():\n",
    "    axs[0].plot(y, row, marker = \"o\", label = len_path)\n",
    "    axs[0].set_title(\"unfinished paths\")\n",
    "    axs[0].legend(title=\"path length\", loc = \"upper left\")\n",
    "    axs[0].xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    len_path += 1\n",
    "\n",
    "len_path = 2\n",
    "y = [x for x in range (max_length)]\n",
    "for it, row in df_pf_similarity.iterrows():\n",
    "    axs[1].plot(y, row, marker = \"o\", label = len_path)\n",
    "    axs[1].set_title(\"finished paths (final article ommited)\")\n",
    "    axs[1].legend(title=\"path length\", loc = \"upper left\")\n",
    "    axs[1].xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    len_path += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a clear difference between the finished and unfinished paths, unsfinished paths stagnating to around 0.3 semantic similarity to the target before being abandonned, while finished paths being at 0.5 similarity to the target on the article before the target.\n",
    "We can also observe that paths which finish in a shorter number of articles on average start at a semantically closer article to the target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
